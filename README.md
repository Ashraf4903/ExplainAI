It is designed to work with any domain â€” technology, science, business, or conceptual topics â€” without requiring manual video editing.

 -## **LLM-Driven Blueprint Generation**
  - Primary: **Groq (LLaMA-3)**
  - Fallback: **Google Gemini**
  - Automatically decides scene count (3â€“6)

-## **Neural Voice Narration**
  - Uses **Edge-TTS (Microsoft Neural Voices)**
  - Natural, human-like speech
  - No paid TTS APIs required

-## **Smart Diagram Rendering**
  - Auto-generated flow diagrams (`A -> B -> C`)
  - Dynamic layout to prevent overlaps
  - Blueprint-style visuals

-## **Animated Video Output**
  - Subtle Ken-Burns style motion
  - Fade-in / fade-out transitions
  - Auto-synced visuals with narration

-## **Background Music Support**
  - Optional royalty-free background music
  - Automatically mixed at low volume

-## **Editable Scene Workflow**
  - Edit narration, visuals, and timing before rendering

-## **Production-Safe Rendering**
  - Audio trimmed after effects
  - Prevents MoviePy timing crashes
  - Automatic cleanup of temporary files

-## **Downloadable Script draft**
  - The narration script, visual script and the duration can be downloaded in CSV format 


ğŸ”‘ API Keys Setup

Create a secrets.json file in the project root:

{
  "groq_api_key": "YOUR_GROQ_API_KEY",
  "gemini_api_key": "YOUR_GEMINI_API_KEY"
}

Supported Models

Groq â†’ Primary (fast, free tier)

Gemini â†’ Automatic fallback

## ğŸš€ How to Run

1.  **Clone the Repository**
    ```bash
    git clone <repo-url>
    cd ExplainAI
    ```

2.  **Install Requirements**
    ```bash
    pip install -r requirements.txt
    ```
    *(Dependencies: streamlit, moviepy, groq, google-generativeai, edge-tts, pillow,imageio-ffmpeg)*

    âš ï¸ FFmpeg is required for MoviePy
    Make sure itâ€™s installed and available in PATH.

3.  **Configure API Keys**
    Create a `secrets.json` file in the root directory:
    ```json
    {
      "groq_api_key": "gsk_...",
      "gemini_api_key": "AIza..."
    }
    ```

4.  **Launch Application**
    ```bash
    streamlit run app.py
    ```

- **How It Works**
  -Enter a topic (e.g. OAuth 2.0 Flow)
  -Click Draft Blueprint
  -AI generates a multi-scene explanation
  -Edit narration or visuals if needed
  -Click Render Final Video
  -MP4 video is generated and displayed

## âš ï¸ Challenges Solved

* **Audio/Video Desync:** Early versions crashed when video effects (Zoom) extended clip duration beyond the audio file. **Solution:** Refactored the rendering pipeline to strictly bind clip duration to `audio_clip.duration` before applying effects.
* **JSON Parsing Errors:** LLMs often output "chatty" text. **Solution:** Implemented a regex-based `clean_json_output()` middleware to strip Markdown before parsing.
* **Text Appears Crossed / Struck:**Horizontal grid lines removed.Text rendering remains clean
* **MoviePy Audio Timing Errors:** Audio is trimmed after effects.Prevents Accessing time t= crashes

## **Future Scope**
* Add support for multi-speaker dialogue.
* Implement "Code Block" visualization for programming topics.
* More Visualtion editing options can be added with a custom details box for user
* Scene-level camera motion
* Dockerized deployment
* Cloud rendering support


## **System Architecture**
User Topic
   â†“
Groq (LLaMA-3)  â†’  Gemini (Fallback)
   â†“
Scene Blueprint (JSON)
   â†“
Edge-TTS Narration
   â†“
Diagram Renderer (PIL)
   â†“
Animated Video (MoviePy)
   â†“
Final MP4 Output

## **Project Structure**
ExplainAI-Pro/
â”‚
â”œâ”€â”€ app.py                # Main Streamlit application
â”œâ”€â”€ secrets.json          # API keys (not committed)
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ bg_music.mp3      # Optional background music
â”œâ”€â”€ README.md


ğŸ‘¨ **Author**
- Ashraf Pathan